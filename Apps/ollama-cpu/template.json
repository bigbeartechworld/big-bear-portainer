{
  "version": "3",
  "templates": [
    {
      "id": 130,
      "type": 2,
      "title": "Ollama - CPU",
      "name": "ollama-cpu",
      "description": "Get up and running with Llama 3, Mistral, Gemma, and other large language models.",
      "note": "LLMs inference server with OpenAI compatible API",
      "categories": ["BigBearCasaOS", "selfhosted"],
      "platform": "linux",
      "logo": "https://cdn.jsdelivr.net/gh/selfhst/icons/png/ollama.png",
      "image": "ollama/ollama:0.12.5",
      "repository": {
        "url": "https://github.com/bigbeartechworld/big-bear-casaos",
        "stackfile": "Apps/ollama-cpu/docker-compose.yml"
      },
      "env": [
    {"name": "PORT", "label": "PORT", "description": "Environment variable for PORT", "default": "11434"}
      ],
      "ports": [    "11434:11434"],
      "volumes": [
    {"container": "/root/.ollama"}
      ],
      "labels": [
        {"name": "maintainer", "value": "BigBearTechWorld"},
        {"name": "version", "value": "0.12.5"},
        {"name": "source", "value": "big-bear-casaos"}
      ],
      "administrator_only": false
    }
  ]
}
