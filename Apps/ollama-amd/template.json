{
  "version": "3",
  "templates": [
    {
      "id": 131,
            "type": 3,
      "title": "Ollama - AMD",
      "name": "ollama-amd",
      "description": "Get up and running with Llama 3, Mistral, Gemma, and other large language models.",
      "note": "LLMs inference server with OpenAI compatible API",
      "categories": ["BigBearCasaOS", "selfhosted"],
      "platform": "linux",
      "logo": "https://cdn.jsdelivr.net/gh/selfhst/icons/png/ollama.png",
      "repository": {
        "url": "https://github.com/bigbeartechworld/big-bear-portainer",
        "stackfile": "Apps/ollama-amd/docker-compose.yml"
      },
      "env": [
    {"name": "PORT", "label": "PORT", "description": "Environment variable for PORT", "default": "11434"}
      ]
    }
  ]
}
